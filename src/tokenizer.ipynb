{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mininlp.data import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'%', 'x', 'd', '6', 'z', '\"', '<pad>', '<mask>', 'k', \"'\", 'X', 'R', '\\\\', '9', 'T', 'S', '=', 'M', ' ', 'y', 'B', '-', '\\t', 'l', 'o', '(', 'Y', '\\x0b', '<unk>', ',', ')', 'g', 'v', '|', 'j', 'q', '\\n', '1', 'C', '<', 'c', 'm', 'b', 'f', 'a', 't', '.', '2', '?', '{', '3', 'H', 'A', '4', '`', '~', '*', 'J', '/', ':', '\\r', 'i', '+', 'h', 'w', '&', '!', '>', '<eos>', 'W', 's', '7', '5', '\\x0c', 'e', 'Z', 'K', 'P', 'V', 'u', 'F', '@', 'N', '$', 'r', '8', '#', 'E', '[', '_', ']', '^', 'Q', 'D', '<sos>', 'p', 'U', 'n', ';', '0', 'L', 'G', 'O', 'I', '}'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Token are every ascii character and special tokens for start of sentence, \n",
    "end of sentence, padding, unknown and mask.\"\"\"\n",
    "from mininlp.data import assci_tokens\n",
    "print(assci_tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = assci_tokens()\n",
    "tokenizer = Tokenizer(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test if the tokenizer is able to convert tokens to ids and vice versa.\"\"\"\n",
    "\n",
    "for id in tokenizer._tokens:\n",
    "    assert tokenizer._token_ids[tokenizer._tokens[id]] == id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World! \n",
      "Lovely day, isn't it?\n",
      "tensor([ 51,  74,  23,  23,  24,  29,  18,  69,  24,  84,  23,   2,  66,  18,\n",
      "         36, 100,  24,  32,  74,  23,  19,  18,   2,  44,  19,  29,  18,  61,\n",
      "         70,  97,   9,  45,  18,  61,  45,  48], dtype=torch.int32)\n",
      "['H', 'e', 'l', 'l', 'o', ',', ' ', 'W', 'o', 'r', 'l', 'd', '!', ' ', '\\n', 'L', 'o', 'v', 'e', 'l', 'y', ' ', 'd', 'a', 'y', ',', ' ', 'i', 's', 'n', \"'\", 't', ' ', 'i', 't', '?']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Test if the tokenizer is able to encode and decode a string.\"\"\"\n",
    "\n",
    "test_string = \"Hello, World! \\nLovely day, isn't it?\"\n",
    "\n",
    "test_encoded = tokenizer.encode(test_string)\n",
    "test_decoded = tokenizer.decode(test_encoded)\n",
    "assert test_string == \"\".join(test_decoded)\n",
    "\n",
    "print(test_string)\n",
    "print(test_encoded)\n",
    "print(test_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test if the tokenizer is able to save and load itself.\"\"\"\n",
    "\n",
    "tokenizer.save(\"tokenizer\")\n",
    "\n",
    "tokenizer2 = Tokenizer()\n",
    "tokenizer2.load(\"tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test if the loaded tokenizer is the same as the original tokenizer.\"\"\"\n",
    "\n",
    "assert tokenizer._tokens == tokenizer2._tokens\n",
    "assert tokenizer._token_ids == tokenizer2._token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'e', 'l', 'l', 'o', ',', ' ', 'W', 'o', 'r', 'l', 'd', '!', ' ', '\\n', 'L', 'o', 'v', 'e', 'l', 'y', ' ', 'd', 'a', 'y', ',', ' ', 'i', 's', 'n', \"'\", 't', ' ', 'i', 't', '?']\n",
      "tensor([ 51,  74,  23,  23,  24,  29,  18,  69,  24,  84,  23,   2,  66,  18,\n",
      "         36, 100,  24,  32,  74,  23,  19,  18,   2,  44,  19,  29,  18,  61,\n",
      "         70,  97,   9,  45,  18,  61,  45,  48], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Test if the orginal tokenizer and the loaded one encodes and decodes a string to the same tokens ids.\"\"\"\n",
    "\n",
    "test_encoded = tokenizer2.encode(test_string)\n",
    "test_decoded = tokenizer.decode(test_encoded)\n",
    "assert test_string == \"\".join(test_decoded)\n",
    "\n",
    "test_encoded = tokenizer.encode(test_string)\n",
    "test_decoded = tokenizer2.decode(test_encoded)\n",
    "assert test_string == \"\".join(test_decoded)\n",
    "\n",
    "print(test_decoded)\n",
    "print(test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mininlp.data import SequenceDataset\n",
    "\n",
    "encoded_document = tokenizer.tokenize_document(\"../data/anna.txt\")\n",
    "dataset = SequenceDataset('../data/anna.txt', tokenizer, 32, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,\n",
       "           6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,\n",
       "           6,  94,  18, 103], dtype=torch.int32),\n",
       " tensor([  6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,\n",
       "           6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,\n",
       "           6,  18, 103,  97], dtype=torch.int32))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<sos>', 'h', 'i', 'c', 'h', ',', ' ', 't', 'h'] ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'h', 'i', 'c', 'h', ',', ' ', 't', 'h', 'o']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<sos>', ' ', 'H', 'e', ' ', 'a', 'l', 'w', 'a', 'y', 's', ' ', 't', 'o', 'o', 'k', ','] ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', ' ', 'H', 'e', ' ', 'a', 'l', 'w', 'a', 'y', 's', ' ', 't', 'o', 'o', 'k', ',', ' ']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<sos>', ' ', 'I', ' ', 'd'] ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', ' ', 'I', ' ', 'd', 'o']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(dataset[50][0]), tokenizer.decode(dataset[50][1]))\n",
    "print(tokenizer.decode(dataset[46][0]), tokenizer.decode(dataset[46][1]))\n",
    "print(tokenizer.decode(dataset[5][0]), tokenizer.decode(dataset[5][1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
