{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mininlp.data import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'z', 'X', '?', 'm', '<', '2', '5', 'T', \"'\", '*', 'n', 'g', '>', '8', 'k', '9', 'Q', ')', '^', '<sos>', 'a', 'Z', 'J', '$', 'f', '\\x0c', 'v', 'Y', '3', 'r', '&', '0', '7', 'b', ';', '4', '<eos>', 'A', '6', 'N', '<unk>', 'F', '\\r', 'u', '\"', 'p', 'j', '/', 'V', ':', '|', '\\t', 'e', 'M', '`', 'R', '-', ',', 'I', 'O', 'l', 'L', '=', 'c', 'K', '+', 'w', '{', '.', 's', '@', 'H', ' ', 'B', '#', 'i', 'h', 'D', '\\\\', 'o', '}', '_', '~', '%', ']', 'E', '<pad>', 'S', '<mask>', 'y', 't', '(', '\\x0b', '!', 'P', 'U', '[', 'x', 'C', '\\n', 'd', 'G', 'W', 'q', '1'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Token are every ascii character and special tokens for start of sentence, \n",
    "end of sentence, padding, unknown and mask.\"\"\"\n",
    "from mininlp.data import assci_tokens\n",
    "print(assci_tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = assci_tokens()\n",
    "tokenizer = Tokenizer(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test if the tokenizer is able to convert tokens to ids and vice versa.\"\"\"\n",
    "\n",
    "for id in tokenizer._tokens:\n",
    "    assert tokenizer._token_ids[tokenizer._tokens[id]] == id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World! \n",
      "Lovely day, isn't it?\n",
      "tensor([ 71,  52,  60,  60,  79,  57,  72, 102,  79,  29,  60, 100,  93,  72,\n",
      "         99,  61,  79,  26,  52,  60,  89,  72, 100,  20,  89,  57,  72,  75,\n",
      "         69,  10,   8,  90,  72,  75,  90,   2], dtype=torch.int32)\n",
      "['H', 'e', 'l', 'l', 'o', ',', ' ', 'W', 'o', 'r', 'l', 'd', '!', ' ', '\\n', 'L', 'o', 'v', 'e', 'l', 'y', ' ', 'd', 'a', 'y', ',', ' ', 'i', 's', 'n', \"'\", 't', ' ', 'i', 't', '?']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Test if the tokenizer is able to encode and decode a string.\"\"\"\n",
    "\n",
    "test_string = \"Hello, World! \\nLovely day, isn't it?\"\n",
    "\n",
    "test_encoded = tokenizer.encode(test_string)\n",
    "test_decoded = tokenizer.decode(test_encoded)\n",
    "assert test_string == \"\".join(test_decoded)\n",
    "\n",
    "print(test_string)\n",
    "print(test_encoded)\n",
    "print(test_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test if the tokenizer is able to save and load itself.\"\"\"\n",
    "\n",
    "tokenizer.save(\"tokenizer\")\n",
    "\n",
    "tokenizer2 = Tokenizer()\n",
    "tokenizer2.load(\"tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test if the loaded tokenizer is the same as the original tokenizer.\"\"\"\n",
    "\n",
    "assert tokenizer._tokens == tokenizer2._tokens\n",
    "assert tokenizer._token_ids == tokenizer2._token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'e', 'l', 'l', 'o', ',', ' ', 'W', 'o', 'r', 'l', 'd', '!', ' ', '\\n', 'L', 'o', 'v', 'e', 'l', 'y', ' ', 'd', 'a', 'y', ',', ' ', 'i', 's', 'n', \"'\", 't', ' ', 'i', 't', '?']\n",
      "tensor([ 71,  52,  60,  60,  79,  57,  72, 102,  79,  29,  60, 100,  93,  72,\n",
      "         99,  61,  79,  26,  52,  60,  89,  72, 100,  20,  89,  57,  72,  75,\n",
      "         69,  10,   8,  90,  72,  75,  90,   2], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Test if the orginal tokenizer and the loaded one encodes and decodes a string to the same tokens ids.\"\"\"\n",
    "\n",
    "test_encoded = tokenizer2.encode(test_string)\n",
    "test_decoded = tokenizer.decode(test_encoded)\n",
    "assert test_string == \"\".join(test_decoded)\n",
    "\n",
    "test_encoded = tokenizer.encode(test_string)\n",
    "test_decoded = tokenizer2.decode(test_encoded)\n",
    "assert test_string == \"\".join(test_decoded)\n",
    "\n",
    "print(test_decoded)\n",
    "print(test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mininlp.data import SequenceDataset\n",
    "\n",
    "encoded_document = tokenizer.tokenize_document(\"../data/anna.txt\")\n",
    "dataset = SequenceDataset('../data/anna.txt', tokenizer, 32, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([86, 86, 86, 86, 19, 52, 29, 72, 69, 66, 52, 52, 45, 72, 79, 24, 72, 76,\n",
       "         75, 69, 72, 69, 63, 89, 90, 76, 52, 57, 72, 10, 79, 90],\n",
       "        dtype=torch.int32),\n",
       " tensor([86, 86, 86, 86, 52, 29, 72, 69, 66, 52, 52, 45, 72, 79, 24, 72, 76, 75,\n",
       "         69, 72, 69, 63, 89, 90, 76, 52, 57, 72, 10, 79, 90, 72],\n",
       "        dtype=torch.int32))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<pad>', '<pad>', '<pad>', '<sos>', 'o', 'w', ' ', 'h', 'e', 'l', 'p', ' ', 'h', 'i', 'm', '?', ' ', 'W', 'h', 'a', 't', '\\n', 'c', 'a', 'n', ' ', 'I', ' ', 's', 'a', 'y'] ['<pad>', '<pad>', '<pad>', '<pad>', 'o', 'w', ' ', 'h', 'e', 'l', 'p', ' ', 'h', 'i', 'm', '?', ' ', 'W', 'h', 'a', 't', '\\n', 'c', 'a', 'n', ' ', 'I', ' ', 's', 'a', 'y', ' ']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<sos>', 'd', ' ', 'h', 'a', 'n', 'd', 's', 'o', 'm', 'e', ',', ' '] ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'd', ' ', 'h', 'a', 'n', 'd', 's', 'o', 'm', 'e', ',', ' ', 'w']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<sos>', ' ', 'M', 'i', 'h', 'a', 'i', 'l', ' ', 'V', 'a'] ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', ' ', 'M', 'i', 'h', 'a', 'i', 'l', ' ', 'V', 'a', 's']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(dataset[50][0]), tokenizer.decode(dataset[50][1]))\n",
    "print(tokenizer.decode(dataset[46][0]), tokenizer.decode(dataset[46][1]))\n",
    "print(tokenizer.decode(dataset[5][0]), tokenizer.decode(dataset[5][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DTransformer(\n",
       "  (_embedding): Embedding(\n",
       "    (_token_embedding): Embedding(128, 512)\n",
       "  )\n",
       "  (_decoders): ModuleList(\n",
       "    (0-5): 6 x Decoder(\n",
       "      (_laynorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_laynorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_mmha): MultiHeadAttention(\n",
       "        (_projection): ModuleList(\n",
       "          (0-2): 3 x Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (_reprojection): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_mha): MultiHeadAttention(\n",
       "        (_projection): ModuleList(\n",
       "          (0-2): 3 x Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (_reprojection): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_ff): FeedForward(\n",
       "        (_laynorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (_dropout): Dropout(p=0.2, inplace=False)\n",
       "        (_ff): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_lang_head): LanguageHead(\n",
       "    (_projection): Linear(in_features=512, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mininlp.transformer import DTransformer\n",
    "import json\n",
    "\n",
    "VERSION = 0.1\n",
    "MODEL_NAME = f'decoder_transformer_v{VERSION}'\n",
    "config = json.load(open(f\"../models/{MODEL_NAME}.json\"))\n",
    "\n",
    "model = DTransformer(\n",
    "    config['layers'], \n",
    "    config['embedding_dim'], \n",
    "    128, \n",
    "    config['seq_len'], \n",
    "    config['heads'], \n",
    "    config['factor'],\n",
    "    True)\n",
    "state_dict = torch.load(f\"../models/{MODEL_NAME}.pt\")\n",
    "model.load_state_dict(state_dict)\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.load(\"../models/tokenizer.pkl\")\n",
    "dataset = SequenceDataset('../data/anna.txt', tokenizer, config['seq_len'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    input = dataset[0][0].unsqueeze(0)\n",
    "    output = model(input.to('cuda'))\n",
    "    probs = F.softmax(output[0, -1, :], dim=0)\n",
    "    probs = probs.detach().cpu()\n",
    "    \n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.bar(tokenizer.decode(torch.tensor(range(len(probs)))), probs)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.decode(dataset[0][0])\n",
    "text += [\"<msk>\"]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prompt = dataset[0][0].unsqueeze(0).to('cuda')\n",
    "    text += tokenizer.decode(model.generate(prompt, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos>to the usual report, he had the most innocent and inoffensive\n",
      "air. No one, looking at his white hands, with their swollen veins and\n",
      "long fingers, so softly stroking the edges of the white paper that lay\n",
      "before him, and at the air of weariness with which his head drooped on\n",
      "one side, would have suspected that in a few minutes a torrent of words\n",
      "would flow from his lips that would arouse a fearful storm, set the\n",
      "members shouting and attacking one another, and force the president to\n",
      "call for order. When the report was <msk>a slal the room, who had palent he neccept mishappy. You\n",
      "comment quieter, Sergey Ivanovitch's earful to see for the interest of a heart,\n",
      "Levin had completely his great hand to him. Mihainin? Scrumpost cond of the Tvildent\n",
      "would not hungerstand, it when he had no gone the horses words himself: \"Well, not\n",
      "expectively. My thinking! why I have his last about founded?\"\n",
      "\n",
      "\"Why go?\" he said, simply glooking to be. \"Shere, you've.\" shall grunning said. When\n",
      "he he could symiliately?\" said smiled from Kitty, as turned on the loudh understand,\n",
      "yet as heigh to peoplee for and morning senting any, was the\n",
      "glass, that was vurse....\n",
      "lightched plentend. Junifully, try brindler, aftered turnat.\n",
      "\n",
      "Chrow Hew how voicely, taking, what\n",
      "as how Betrilian, went from, waiteress (Fosly renting it merclory rivt traitim double disnger\n",
      "and the twcless forcty, dicledle a lander start of her flace, not pressing yelvnip on deflusion;\n",
      "then is from, both he hadrd fealu.\n",
      "\n",
      "VAFDIvn's had last bett Sergey.\"\n",
      "\n",
      "Becatever her an\n"
     ]
    }
   ],
   "source": [
    "text = [t for t in text if t != \"<pad>\"]\n",
    "print(\"\".join(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
